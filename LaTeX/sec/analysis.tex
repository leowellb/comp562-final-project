\section{Analysis}

The performance of the models was ranked based on two criteria: maximizing the overall accuracy while minimizing the false-negative rate. Although overall accuracy was the most important factor in determining the best model, minimizing false-negatives is also very important as we do not want to misclassify a potential illicit drug abuser as someone who was clean.\\

For each technique, we trained four models through 4-fold cross-validation. To find the overall accuracy, we summed the correct predictions for each of the four cross-validation models and returned the 95\% confidence interval for that technique. To find the overall confusion matrix, we added each of the four confusion matrices together and reported their true negative, true positive, false negative, and false-positive rates.

\subsection{Logistic Regression}

Our logistic regression model shows a good baseline with an accuracy of 0.8053. The standard data set outperformed the fully categorized data set by a statistically significant margin, implying that quantifying the data does improve our models.

\begin{table}[h!]
\begin{tabular}{|c|c|c|}
\hline
 & Standard & \begin{tabular}[c]{@{}c@{}}Fully \\ Categorized\end{tabular} \\
\hline
\begin{tabular}[c]{@{}c@{}} False \\ Positives\end{tabular} & 16.46\% & 17.85\% \\
\hline
\begin{tabular}[c]{@{}c@{}} False \\ Negatives\end{tabular} & 24.21\% & 29.55\% \\
\hline
Accuracy & \begin{tabular}[c]{@{}c@{}}0.8053 \\ (+/- 0.0200)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.7761 \\ (+/- 0.0151)\end{tabular}\\
\hline
\end{tabular}
\end{table} 

\subsection{Naive Bayes}

For Naive Bayes models, BernoulliNB outclassed GaussianNB in both accuracy and minimizing false negatives. Most of our dataset’s scalar parameters are quantified around 0, which makes those parameters a good fit for binarization. Coupled with having the majority of parameters be one-hot encoded makes BernoulliNB a better model.

\begin{table}[h!]
\begin{tabular}{|c|c|c|}
\hline
 & GaussianNB & BernoulliNB \\
 \hline
\begin{tabular}[c]{@{}c@{}}False \\ Positives\end{tabular} & 15.42\% & 21.49\% \\
\hline
\begin{tabular}[c]{@{}c@{}}False \\ Negatives\end{tabular} & 34.33\% & 17.78\% \\
\hline
Accuracy & \begin{tabular}[c]{@{}c@{}}0.7724 \\ (+/- 0.0270)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.7995 \\ (+/- 0.0143)\end{tabular}\\
\hline
\end{tabular}
\end{table}

\subsection{Support Vector Machines}

For the SVM models, most kernels worked pretty well, with linear, rbf, and sigmoid kernels all showing roughly the same accuracy performance. Linear kernels had a small increase in false-negative rates, but that may simply be due to variation in the data. Polynomial kernels produced notably poor fits. Out of all the polynomial dimensions tested, only the 2-D kernel’s accuracy came close. 3-D, 4-D, and 5-D kernels all performed very poorly, with accuracies of 0.6223, 0.6122, and 0.6122 respectively. Interestingly, 3-D, 4-D and 5-D kernels gravitated heavily towards predicting the respondent as negative, with 4-D and 5-D kernels always predicting negative, implying that the data set is not fit for a polynomial transformation.

\begin{table}[h!]
\begin{tabular}{|c|c|c|}
 \hline
 & Linear & Rbf \\ \hline
\begin{tabular}[c]{@{}c@{}}False \\ Positives\end{tabular} & 16.64\% & 17.85\% \\ \hline
\begin{tabular}[c]{@{}c@{}}False \\ Negatives\end{tabular} & 23.26\% & 21.89\% \\ \hline
Accuracy & \begin{tabular}[c]{@{}c@{}}0.8080 \\ (+/- 0.0228)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.8058 \\ (+/- 0.0238)\end{tabular} \\ \hline
 & Sigmoid & Poly 2 \\ \hline
\begin{tabular}[c]{@{}c@{}}False \\ Positives\end{tabular} & 17.24\% & 14.30\% \\ \hline
\begin{tabular}[c]{@{}c@{}}False \\ Negatives\end{tabular} & 21.61\% & 34.06\% \\ \hline
Accuracy & \begin{tabular}[c]{@{}c@{}}0.8106 \\ (+/- 0.0282)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.7804 \\ (+/- 0.0417)\end{tabular} \\  \hline
\end{tabular}
\end{table}

\subsection{Neural Networks}

Unlike the other models, our neural networks did not return the same performance statistics every time we trained the model. This is most likely due to Keras’ random initiation of starting node values, resulting in a different model each time. We were unable to get our neural networks to outperform our simpler models like Logistic Regression or Naive Bayes. In conclusion, due to our dataset’s compatibility with traditional machine learning models and relatively small sample size, it is difficult for the neural network to outperform traditional models without overfitting.

\begin{table}[h!]
\begin{tabular}{|c|c|c|}
\hline
 & Basic & \begin{tabular}[c]{@{}c@{}}More Layers \\ (tanh/ReLU)\end{tabular} \\
 \hline
\begin{tabular}[c]{@{}c@{}}False \\ Positives\end{tabular} & 20.36\% & 20.19\% \\
\hline
\begin{tabular}[c]{@{}c@{}}False \\ Negatives\end{tabular} & 29.41\% & 31.19\% \\
\hline
Accuracy & \begin{tabular}[c]{@{}c@{}}0.7613 \\ (+/- 0.0432)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.7554 \\ (+/- 0.0324)\end{tabular} \\
\hline
 & \begin{tabular}[c]{@{}c@{}}More Layers \\ (LeakyReLU)\end{tabular} &  \\
 \hline
\begin{tabular}[c]{@{}c@{}}False \\ Positives\end{tabular} & 19.76\% &  \\
\hline
\begin{tabular}[c]{@{}c@{}}False \\ Negatives\end{tabular} & 29.00\% &  \\
\hline
Accuracy & \begin{tabular}[c]{@{}c@{}}0.7666 \\ (+/- 0.0154)\end{tabular} & \\ \hline
\end{tabular}
\end{table}

\subsection{Cumulative Analysis}

Most of our models were barely able to cross the 80\% accuracy threshold. This accuracy cap may be due to the dataset’s limited size. Our small training set may have contained a lot of variance that is not shown within the given parameters, which led most of our models to return similar performances. Our sigmoid-kernel SVM had the best accuracy, but not by a statistically significant amount. On the other hand, our BernoulliNB had slightly poorer accuracy, but had less variance within its models and kept false negative much lower.
