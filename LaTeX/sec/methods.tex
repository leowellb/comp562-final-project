\section{Methods}

To create the most accurate model, we utilized four different machine learning techniques: Logistic Regression, Naive Bayes, Support Vector Machines (SVM), and Neural Networks.\hspace{1pt}Logistic Regression, Naive Bayes, and SVM were all implemented using the sci-kit-learn machine learning library while the neural network was implemented using Keras. For each technique, we modified the training parameters and documented how the technique and modification affected the results.\hspace{1pt}Unspecified training parameters were set to their default values. Each method was cross-validated using k-fold cross-validation of 4 folds.\hspace{1pt}Due to the original dataset’s uneven distribution of recorded data, we shuffled the data during k-fold validation using random-state: 0. 

\subsection{Logistic Regression}

We made two logistic regression models, one trained on the original dataset, and the other trained on a modified dataset in which every parameter was treated as a categorical variable and one-hot encoded. The purpose of conducting the second test was to observe performance changes by assuming independence between the values of each parameter. (Note: After one-hot encoding each parameter, the dataset contained 2,000+ columns. Due to a large number of columns, the other techniques are trained only on the original dataset.)

\subsection{Naive Bayes}

We made two Naive Bayes models, one assuming feature likelihood is Gaussian distributed, and another assuming feature likelihood is Bernoulli distributed.\hspace{1pt}Because our data is quantified, it is very easy to binarize our data set, making it a good fit for Bernoulli Naive Bayes. We compared the Bernoulli Naive Bayes to the standard Gaussian Naive Bayes to observe any differences in performance.

\begin{figure}[h!]
    \[ P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2}) \]
    \caption{Gaussian Distribution}
    \vspace{1pt}
    \[ P(x_i \mid y) = P(i \mid y)x_i + (1-P(i \mid y))(1-x_i) \]
    \caption{Bernoulli Distribution}
\end{figure}

\subsection{Support Vector Machines (SVM)}

When making SVM’s, we focused our attention on the way different kernels affected our model. We made 7 SVM’s to test 4 different kernel methods: linear, gaussian radial basis function (rbf), sigmoid, and polynomial. The equations for these kernels are shown below:

\vspace{-10pt}

\begin{figure}[h!]
    \[ k(x,y) = x^T y + c \]
    \caption{Linear}
    \vspace{1pt}
    \[ k(x,y) = exp(-\gamma ||x-y||^2) \]
    \caption{Gaussian Radial Basis Function}
    \vspace{1pt}
    \[ k(x,y) = tanh(\alpha x^T y + c) \]
    \caption{Sigmoid}
    \vspace{1pt}
    \[ k(x,y) = (\alpha x^T y + c)^d \]
    \caption{Polynomial}
\end{figure}

For the polynomial kernels, we tested using 2, 3, 4, and 5 dimensions.\hspace{1pt}All other training parameters for the SVM’s are set to their default values.

\subsection{Neural Networks}

Given the structure of our data, our neural networks were all sequential models with a varied number of layers as well as different activation functions.\hspace{1pt}We created a total of three neural networks: a basic one; one with additional layers that utilized a combination of tanh/ReLU activation functions; and one that also had additional layers, but utilized Leaky ReLU as the activation function.\hspace{1pt}We kept the number of epochs and the batch size constant between all three neural networks.\hspace{1pt}For all three networks, we used binary cross-entropy for loss and Adam as the optimizer.\\

Our basic neural network consisted of 4 dense layers with three intermediate layers containing 64, 32, and 16 nodes along with one output layer with one node.\hspace{1pt}The first three used layers used ReLU as the activation function and the final layer used sigmoid.\\

Our second neural network consisted of 6 dense layers, with the 5 intermediate layers containing 128, 64, 32, 16, and 8 nodes and our output layer containing one node.\hspace{1pt}The first three layers used ReLU, the next two used tanh, and the output layer used sigmoid.\\

Lastly, our third neural network used Leaky ReLU as the activation function for the intermediate layers, which had the same structure as our neural network with the exception of additional layers. This time, however, the activation function for the first 5 layers was Leaky ReLU, with our final output layer using sigmoid. 
